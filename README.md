## 1 ç®—æ³•ä»‹ç»

æ­¤é¡¹ç›®é‡‡ç”¨ç»å…¸çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆ DQNï¼ˆDeep Q-Networkï¼‰ç®—æ³•ï¼‰å®ç°è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨é«˜é€Ÿå…¬è·¯ï¼ˆhighwayï¼‰ä¸Šçš„å†³ç­–ä»»åŠ¡ã€‚

### 1.1 æ¦‚è¿°

DQNï¼ˆæ·±åº¦ Q ç½‘ç»œï¼‰æ˜¯ä¸€ç§ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ï¼Œå°†ä¼ ç»Ÿçš„ Q-learning ç®—æ³•ä¸æ·±åº¦ç¥ç»ç½‘ç»œç›¸ç»“åˆã€‚

å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è®©æ™ºèƒ½ä½“ï¼ˆagentï¼‰é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜çš„å†³ç­–ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ã€‚
DQN é€šè¿‡ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥é€¼è¿‘ Q å€¼å‡½æ•°ï¼Œä½¿å¾—ç®—æ³•èƒ½å¤Ÿåœ¨å¤æ‚çš„é«˜ç»´çŠ¶æ€ç©ºé—´ä¸­å·¥ä½œï¼Œä¾‹å¦‚è§†é¢‘æ¸¸æˆæˆ–è‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥é—®é¢˜ã€‚

### 1.2 ç›¸å…³åŸç†

DQN åŸºäº Q-learningç®—æ³•è¿›è¡Œæ”¹è¿›ï¼Œæ˜¯ä¸€ç§å€¼å‡½æ•°æ–¹æ³•ï¼Œç”¨äºä¼°è®¡çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ˆstate-action pairï¼‰çš„æœŸæœ›æœªæ¥å¥–åŠ±ã€‚Q-learning ä½¿ç”¨ Q å€¼å‡½æ•°
Q(s,a) æ¥è¡¨ç¤ºåœ¨çŠ¶æ€
s ä¸‹æ‰§è¡ŒåŠ¨ä½œ
a èƒ½è·å¾—çš„é¢„æœŸç´¯ç§¯å¥–åŠ±ã€‚

#### 1.2.1 Q-learning

Q-learning æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå±äºå€¼å‡½æ•°æ–¹æ³•ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç§ç­–ç•¥ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨æ¯ä¸€ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©çš„åŠ¨ä½œèƒ½å¤Ÿæœ€å¤§åŒ–å…¶é¢„æœŸçš„ç´¯ç§¯å¥–åŠ±ã€‚

Q-learning é€šè¿‡å­¦ä¹ ä¸€ä¸ª Q å‡½æ•°ï¼ˆQ-table æˆ– Q-value functionï¼‰ï¼Œè¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªçŠ¶æ€å’Œä¸€ä¸ªåŠ¨ä½œä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªå€¼ï¼ˆQ å€¼ï¼‰ï¼Œè¡¨ç¤ºåœ¨è¿™ä¸ªçŠ¶æ€ä¸‹æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œèƒ½å¤Ÿè·å¾—çš„é¢„æœŸå¥–åŠ±ã€‚

Q-learning çš„æ›´æ–°å…¬å¼ä¸ºï¼š

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

å…¶ä¸­ï¼š

* s æ˜¯å½“å‰çŠ¶æ€
* a æ˜¯å½“å‰åŠ¨ä½œ
* r æ˜¯æ‰§è¡ŒåŠ¨ä½œ a åå¾—åˆ°çš„å¥–åŠ±
* s' æ˜¯æ‰§è¡ŒåŠ¨ä½œ a åçš„ä¸‹ä¸€çŠ¶æ€
* Î± æ˜¯å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ›´æ–°é€Ÿåº¦
* Î³ æ˜¯æŠ˜æ‰£å› å­ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„é‡è¦æ€§

ä½†æ˜¯åœ¨æ™®é€šçš„Q-learningä¸­ï¼Œå½“çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£ä¸”ç»´æ•°ä¸é«˜æ—¶å¯ä½¿ç”¨Q-Tableå‚¨å­˜æ¯ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹çš„Qå€¼ï¼Œè€Œå½“çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ˜¯é«˜ç»´è¿ç»­æ—¶ï¼Œä½¿ç”¨Q-Tableä¸åŠ¨ä½œç©ºé—´å’ŒçŠ¶æ€å¤ªå¤§ååˆ†å›°éš¾ã€‚
å› æ­¤ï¼ŒDQNåœ¨Q-learningçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚

#### 1.2.2 æ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰

DQN ä½¿ç”¨ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆQ ç½‘ç»œï¼‰æ¥é€¼è¿‘ Q å€¼å‡½æ•°ã€‚ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯çŠ¶æ€ ğ‘  ï¼Œè¾“å‡ºæ˜¯æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„ Q å€¼
ğ‘„(ğ‘ ,ğ‘;ğœƒ)ï¼Œå…¶ä¸­ ğœƒ æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼ŒDQN å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š

* ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰ï¼šæ™ºèƒ½ä½“åœ¨ä¸ç¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ä¸­ï¼Œä¼šç”Ÿæˆå¤§é‡çš„ç»éªŒæ•°æ®ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼‰ã€‚è¿™äº›æ•°æ®è¢«å­˜å‚¨åœ¨ä¸€ä¸ªå›ºå®šå¤§å°çš„å›æ”¾ç¼“å†²åŒºä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDQN ä¼šéšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„ç»éªŒè¿›è¡Œè®­ç»ƒã€‚è¿™ç§åšæ³•æ‰“ç ´äº†ç»éªŒä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ï¼Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚
* ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰ï¼šDQN ç»´æŠ¤ä¸¤ä¸ªç¥ç»ç½‘ç»œï¼šå½“å‰ Q ç½‘ç»œï¼ˆç”¨äºé€‰æ‹©åŠ¨ä½œå’Œæ›´æ–° Q å€¼ï¼‰å’Œç›®æ ‡ Q ç½‘ç»œï¼ˆç”¨äºè®¡ç®—ç›®æ ‡ Q å€¼ï¼‰ã€‚ç›®æ ‡ Q ç½‘ç»œçš„å‚æ•°æ¯éš”ä¸€å®šæ­¥æ•°ä»å½“å‰ Q ç½‘ç»œå¤åˆ¶è€Œæ¥ï¼Œé¿å…äº†ç½‘ç»œå‚æ•°é¢‘ç¹æ›´æ–°å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚

### 1.3 å·¥ä½œæµç¨‹

1. **åˆå§‹åŒ–ï¼š**
   * åˆå§‹åŒ–å½“å‰ Q ç½‘ç»œå’Œç›®æ ‡ Q ç½‘ç»œï¼Œå¹¶éšæœºåˆå§‹åŒ–å®ƒä»¬çš„å‚æ•°ã€‚
   * åˆå§‹åŒ–ç»éªŒå›æ”¾ç¼“å†²åŒºä¸ºç©ºã€‚
2. **ä¸ç¯å¢ƒäº¤äº’ï¼š**
   * åœ¨æ¯ä¸ªæ—¶é—´æ­¥ ğ‘¡ ï¼Œæ™ºèƒ½ä½“æ ¹æ®å½“å‰ Q ç½‘ç»œçš„è¾“å‡ºå’Œ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œã€‚å³ï¼Œä»¥ Îµ çš„æ¦‚ç‡é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰ï¼Œä»¥
     1 âˆ’ ğœ– çš„æ¦‚ç‡é€‰æ‹©å½“å‰ Q ç½‘ç»œè¾“å‡ºçš„æœ€å¤§ Q å€¼å¯¹åº”çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚
   * æ‰§è¡ŒåŠ¨ä½œ ğ‘ ï¼Œè·å¾—å¥–åŠ±
     ğ‘Ÿ å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ ğ‘ â€²ã€‚
   * å°†ç»éªŒ(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒºã€‚
3. **ç»éªŒå›æ”¾ï¼š**
   * ä»å›æ”¾ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„ç»éªŒ(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)
     è¿›è¡Œè®­ç»ƒã€‚
   * å¯¹äºæ¯ä¸ªç»éªŒï¼Œè®¡ç®—ç›®æ ‡ Q å€¼ ğ‘¦ ï¼š

$$
y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})
$$

å…¶ä¸­ï¼Œ
ğ‘„(ğ‘ â€²,ğ‘â€²;ğœƒâˆ’)æ˜¯ç›®æ ‡ç½‘ç»œç»™å‡ºçš„ Q å€¼ï¼Œğœƒâˆ’è¡¨ç¤ºç›®æ ‡ç½‘ç»œçš„å‚æ•°

4. **æ›´æ–° Q ç½‘ç»œï¼š**
   * ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ–å½“å‰ Q ç½‘ç»œè¾“å‡ºçš„ Q å€¼ä¸ç›®æ ‡ Q å€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š

$$
L(\theta) = \mathbb{E} \left[(y - Q(s, a; \theta))^2\right]
$$

* è®¡ç®—æŸå¤±å¹¶æ›´æ–°å½“å‰ Q ç½‘ç»œçš„å‚æ•°ğœƒã€‚

5. **æ›´æ–°ç›®æ ‡ç½‘ç»œï¼š**
   
   * æ¯éš”ä¸€å®šæ­¥æ•°ï¼Œå°†å½“å‰ Q ç½‘ç»œçš„å‚æ•° ğœƒ å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œçš„å‚æ•° ğœƒâˆ’ ã€‚
6. **é‡å¤ï¼š**
   
   * é‡å¤æ­¥éª¤ 2 åˆ°æ­¥éª¤ 5ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°æˆ–å…¶ä»–ç»ˆæ­¢æ¡ä»¶ã€‚

## 2 æœ¬åœ°éƒ¨ç½²

æ³¨æ„ï¼š

Stable-Baselines3 æ”¯æŒ PyTorch >= 1.13ã€‚

Stable-Baselines3çš„å®‰è£…éœ€è¦ Python 3.8+ã€‚

**æ–‡æ¡£è¯´æ˜ï¼š**

* main.py #ä¸»æ–‡ä»¶
* utils æ–‡ä»¶å¤¹
  * train.py #æ¨¡å‹è®­ç»ƒ
  * evaluate.py #æ¨¡å‹è¯„ä¼°
  * visualize.py #å¯è§†åŒ–
* ReadMe.md #æ–‡æ¡£

### 2.1 è™šæ‹Ÿç¯å¢ƒé…ç½®

```
conda create -n env_name python=3.8 # å®‰è£…è™šæ‹Ÿç¯å¢ƒ
activate env_name
pip install stable-baselines3[extra] # å®‰è£…stableâ€”baseline3
pip install highway-env==1.8.2
pip install gym
pip install gymnasium==0.29.1
```
### 2.2 å…·ä½“æ­¥éª¤

#### 2.2.1 æ¨¡å‹è®­ç»ƒ

train (env_name, total_timesteps=20000, save_path="highway_dqn/model")

```
ä½¿ç”¨ Stable-Baselines3 è®­ç»ƒ DQN æ¨¡å‹ã€‚

* env_name: ä»¿çœŸç¯å¢ƒåç§°ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­é€‰ç”¨ highway_env ä¸­çš„ highway-v0
* total_timestepsï¼šè®­ç»ƒæ€»æ­¥æ•°ï¼Œæ ¹æ®ä¸åŒåœºæ™¯è®­ç»ƒéš¾åº¦è¿›è¡Œè°ƒèŠ‚ï¼Œé»˜è®¤ä¸º20000
* save_path:æ¨¡å‹ä¿å­˜è·¯å¾„
* returnï¼šåˆ©ç”¨DQNç®—æ³•è®­ç»ƒå¥½çš„æ¨¡å‹

#### 2.2.2 æ¨¡å‹è¯„ä¼°
```

evaluate(env_name, trained_model)

```
è¯„ä¼°è®­ç»ƒå¥½çš„ DQN æ¨¡å‹æ€§èƒ½ã€‚

* env_name:Gym ç¯å¢ƒåç§°ï¼Œä¾‹å¦‚ "highway-v0"ã€‚
* modelï¼šè®­ç»ƒå¥½çš„ DQN æ¨¡å‹

#### 2.2.3 æ¨¡å‹é¢„æµ‹ä¸å¯è§†åŒ–
```

visualize(env_name, model, step_num = 100)

```
ä½¿ç”¨è®­ç»ƒå¥½çš„ DQN æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶ç”¨highway_env è¿›è¡Œå¯è§†åŒ–ã€‚

* env_name:Gym ç¯å¢ƒåç§°
* modelï¼šè®­ç»ƒå¥½çš„ DQN æ¨¡å‹
* step_num: é¢„æµ‹çš„ä»¿çœŸæ­¥æ•°ï¼Œé»˜è®¤ä¸º 100 æ­¥ã€‚

æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°é˜¶æ®µçš„ average rewardï¼Œaverage lengthï¼Œlearning_rate å’Œ loss ç­‰å›¾å¯ä»¥æ‰“å¼€tensorboardæŸ¥çœ‹ã€‚

åœ¨ç»ˆç«¯è¾“å…¥å¦‚ä¸‹ä»£ç æ‰“å¼€ï¼š
```

tensorboard --logdir â€œè®­ç»ƒæ¨¡å‹logå­˜å‚¨åœ°å€â€

```
_æ³¨æ„ï¼šå­˜å‚¨åœ°å€éœ€è¦ç»å¯¹è·¯å¾„_

## 3 å®Œæ•´Demo
```

from train import train
from evaluate import evaluate
from visualize import visualize

# ç¯å¢ƒé€‰æ‹©

env_name = 'highway-v0'

# æ¨¡å‹è®­ç»ƒ

trained_model = train(env_name, total_timesteps=20000)

# æ¨¡å‹è¯„ä¼°

evaluate(env_name, trained_model)

# æ¨¡å‹å¯è§†åŒ–

visualize(env_name, trained_model, step_num=100)
```
ä»¿çœŸç¯å¢ƒå¦‚ä¸‹å›¾ï¼š

<p align="center">
  <img src="utils/img/highway.png">
</p>

æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

<p align="center">
  <img src="utils/img/img.png">
</p>

åœ¨highway-v0ç¯å¢ƒä¸‹è¿›è¡Œ20000æ¬¡è®­ç»ƒçš„å¹³å‡å¥–åŠ±å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

<p align="center">
  <img src="utils/img/reward_demo.png">
</p>

# highway_RL
